请您充当论文编辑专家，站在论文评审的角度，对论文进行修改，使其更加流畅、优美，并提升可读性。具体要求如下：

1. 简洁的描述方法与结果：用简洁而清晰的语言对论文中的方法和结果进行详细描述，确保评审能够轻松理解论文的研究过程和结论。
2. 优化语言流畅性与逻辑结构：改善论文的语言流畅性，使文章的结构更为清晰、连贯。避免复杂冗长的句子和结构，使得每一部分内容都能自然地过渡到下一部分，提升论文的整体可读性。
3. 避免重复与无关内容：删除不必要的重复和冗余部分，确保每段内容都具有清晰的目的，并与论文的主题紧密相关。

下文是论文的一部分，请你修改它：

# 1  视觉语言模型的研究现状

视觉语言模型（Visual-Language Models, VLMs）结合了计算机视觉与自然语言处理技术，旨在实现图像与文本的多模态信息理解。它们在图像描述生成（Image Captioning, IC）、视觉问答（Visual Question Answering, VQA）和视觉定位（Visual Grounding, VG）等跨模态任务中展现了巨大的潜力。近年来，Transformer架构在自然语言处理和计算机视觉领域的广泛应用，为视觉语言模型的发展提供了强大支持。得益于Transformer的自注意力机制，模型能够同时处理图像与文本数据，为多模态信息理解提供了创新解决方案。因此，基于Transformer的视觉语言模型已逐渐成为多模态任务研究的主流，并在多个应用领域取得了显著进展。

视觉语言模型的早期发展受到BERT（Bidirectional Encoder Representations from Transformers）在自然语言处理领域成功的启发。随着BERT及其变种模型的广泛应用，许多研究者开始将其架构扩展到图像与文本结合的跨模态任务中。Visual-BERT和ViLBERT是这一探索的代表性模型，它们首次将Transformer架构应用于视觉语言任务，尤其在掩码语言建模（Masked Language Modeling）和图像-文本匹配（Image-Text Matching）任务中取得了显著成果。这些模型通过Transformer的自注意力机制，成功捕捉图像与文本之间的相互关系，促进了模型对多模态信息的理解。这些开创性工作为后续研究提供了宝贵的经验，并为进一步的发展奠定了坚实的基础。

随着视觉语言模型应用需求的不断增长，研究者提出了四种主流架构类型。第一类是基于对比学习的模型，这类模型通过对正负样本的对比，推动图像-文本对的嵌入空间对齐，从而显著提高跨模态对齐效果。第二类是基于掩码图像建模的模型，这类模型通过部分遮掩输入的图像或文本信息，迫使模型在恢复缺失内容时进行推测和重建，提升了推理能力。第三类是基于大语言模型构建的模型，这类模型通常采用大型预训练语言模型（如Llama等）作为骨干，结合图像编码器与语言模型对齐，显著提高图像-文本匹配的准确度，并在多个任务中展现出优异性能。第四类是生成式模型，这些模型不仅能直接生成图像或文本，广泛应用于图像描述和生成等任务，还能为创新应用提供解决方案，如图像生成与编辑。

这四种主流架构为视觉语言模型的发展提供了多种可行的技术路径，从高效的跨模态对齐到创新的图像生成，极大地推动了视觉语言模型的应用与技术进步。值得注意的是，这些架构并非孤立存在，许多模型在设计时巧妙地融合了对比学习、掩码训练与生成策略的优点。接下来，我们将深入探讨每种架构中的一到两个代表性模型，并详细分析其设计思路与应用场景。

参考文献：

Vaswani A., Shazeer N., Parmar N., et al. Attention is All you Need[C]. Conference on Neural Information Processing Systems. 2017: 5998-6008.

Devlin J., Chang M. W., Lee K., et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2019: 4171-4186.

Li L. H., Yatskar M., Yin D., et al. Visualbert: A Simple and Performant Baseline for Vision and Language[EB/OL]. arXiv preprint arXiv:1908.03557, 2019.

Lu J., Batra D., Parikh D., et al. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks[C]. Conference on Neural Information Processing Systems. 2019: 13-23.

Touvron H., Lavril T., Izacard G., et al. Llama: Open and Efficient Foundation Language Models[EB/OL]. arXiv preprint arXiv:2302.13971, 2023.

## 1.1 基于对比学习的VLMs

基于对比学习的视觉语言模型旨在学习图像与文本之间的对齐表示，其核心思想是通过对比损失函数优化模型参数，促使图像和文本在嵌入空间中实现跨模态的语义关联。具体而言，这类模型将正样本对（由图像及其对应的真实文本描述组成）与负样本对（将同一张图像与描述其他图像的文本配对）进行对比学习。通过区分正负样本对，这些视觉语言模型能够将语义相关的图像和文本映射到嵌入空间中的相近位置，而不相关的样本则被推向较远的位置。这种方法有助于提高跨模态对齐的精度，使模型能够更好地理解图像与文本之间的关系。

CLIP（Contrastive Language-Image Pre-Training）是这一领域的代表性模型。CLIP利用对比学习框架训练视觉和文本编码器，使图像和其对应的文本描述在嵌入空间中具有相似的向量表示。CLIP的训练数据集包含了四亿对来自网络的图像和文本。这些大规模的数据使得CLIP能够学习到丰富的视觉和语言特征，并在多个任务中展现出强大的性能，尤其是在零样本分类任务中表现卓越。以基于ResNet-101的CLIP模型为例，它在零样本分类准确率上达到了76.2%，并且在多个鲁棒性基准测试中，CLIP模型超越了经过监督训练的ResNet模型。

SigLIP（Signature Language Image Pre-training）是一种基于CLIP的改进模型，其主要创新体现在损失函数的设计上。与CLIP使用的信息噪声对比估计（Information Noise Contrastive Estimation，InfoNCE）损失函数不同，SigLIP采用了基于二元交叉熵的噪声对比估计（Noise Contrastive Estimation，NCE）损失函数。通过这一调整，SigLIP能够在较小批量数据下仍然展现出优异的零样本性能，从而减少了对大规模数据的依赖。

Llip（Latent Language Image Pretraining）模型进一步改进了图像与文本之间的关联方式。考虑到一张图像可以用多种不同的方式进行描述，Llip提出了一种新的条件编码机制，通过交叉注意力模块来根据目标描述调整图像的编码。这种方法的核心在于通过动态调整图像编码，使得每个图像可以针对不同的描述生成不同的编码表示，从而提高了编码的多样性和表达能力。Llip通过这种条件编码机制，能够灵活地适应各种任务需求，并进一步推动了跨模态学习的进展。

参考文献：

Radford A., Kim W. J., Hallacy C., et al. Learning Transferable Visual Models From Natural Language Supervision[C]. International Conference on Machine Learning. 2021: 8748-8763.

He K., Zhang X., Ren S., et al. Deep Residual Learning for Image Recognition[C]. IEEE Conference on Computer Vision and Pattern Recognition. 2016: 770-768.

Zhai X., Mustafa B., Kolesnikov B., et al. Sigmoid Loss for Language Image Pre-Training[C]. IEEE/CVF International Conference on Computer Vision. 2023: 11941-11952.

Lavoie S., Kirichenko P., Ibrahim M., et al. Modeling Caption Diversity in Contrastive Vision-Language Pretraining. International Conference on Machine Learning. 2024: 26070-26084.

Gutmann M., Hyvärinen A. Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models[C]. International Conference on Artificial Intelligence and Statistics. 2010: 297-304.

Oord A., Li Y., Vinyals O. Representation Learning with Contrastive Predictive Coding[EB/OL]. arXiv preprint arXiv:1807.03748, 2018.

## 1.2 基于掩码图像建模的VLMs

在大语言模型的早期研究中，BERT通过掩码语言建模（Masked Language Modeling, MLM）技术，预测句子中缺失的词元，从而显著提升了在多种自然语言处理任务中的表现。受到BERT文本掩码技术的启发，视觉语言模型领域也开始采用类似的掩码策略，特别是在图像编码方面。例如，MAE（Masked Autoencoders）和I-JEPA（Image-Joint Encoding and Pretraining Architecture）便是通过掩码图像建模（Masked Image Modeling, MIM）技术来训练图像编码器。这些基于掩码的视觉语言模型的核心思想是，通过随机去除图像输入的部分区域，迫使模型在缺失信息的情况下进行推理，从而提升其跨模态信息的理解能力。

FLAVA（Foundational Language and Vision Alignment）是基于掩码方法的典型模型。该模型架构包括三个核心组件，均基于Transformer框架，并针对不同模态进行了优化。具体而言，图像编码器采用ViT（Vision Transformer）将图像分割为图像块，并通过线性嵌入和Transformer表示进行处理，处理过程中还会附带一个分类标记（[CLSI]）。文本编码器则使用Transformer对文本进行标记化，将文本嵌入为向量并进行上下文处理，输出隐藏状态向量，并附带一个分类标记（[CLST]）。这两个编码器都采用了掩码训练方法。多模态编码器则融合了图像和文本编码器的隐藏状态，借助线性投影和跨模态注意力机制有效整合视觉与文本信息，并引入额外的多模态分类标记（[CLSM]）。FLAVA模型通过结合多模态和单模态的掩码建模损失，并辅以对比学习目标，展现了卓越的多功能性和有效性。在7000万对公开图像和文本数据上进行预训练后，FLAVA在35个涵盖视觉、语言和多模态的基准任务中取得了最先进的性能，展示了其强大的跨领域信息理解与整合能力。

尽管FLAVA模型在多模态任务中表现出色，但其局限性在于依赖于预训练的视觉编码器（例如dVAE）。为了解决这一问题，Kwon等人提出了MaskVLM（Masked Vision-Language Model）。与FLAVA不同，MaskVLM不依赖于预训练的视觉编码器，而是直接在像素空间和文本标记空间上应用掩码策略。这一创新使得MaskVLM能够在没有外部视觉编码器的帮助下，直接处理图像和文本输入。MaskVLM通过信息流动机制，允许一个模态中的信息有效传递到另一个模态，从而使得模型能够在视觉和语言两个模态之间进行有效的联合学习。例如，在文本重构任务中，MaskVLM利用图像编码器的信息来辅助重构文本内容；而在图像任务中，模型则可以利用文本编码器提供的信息来辅助图像的处理和理解。

参考文献：

Devlin J., Chang M. W., Lee K., et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2019: 4171-4186.

He K., Chen X., Xie S., et al. Masked Autoencoders are Scalable Vision Learners[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 15979-15988.

Assran M., Duval Q., Misra I., et al. Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 15619-15629.

Singh A., Hu R., Goswami V., et al. Flava: A Foundational Language and Vision Alignment Model[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 15617-15629.

Dosvitskiy A., Beyer L., Kolesnikov A., et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale[EB/OL]. arXiv preprint arXiv:2010.11929, 2020.

Zhang M., Jiang S., Cui Z., et al. D-VAE: A Variational Autoencoder for Directed Acyclic Graphs[C]. Conference on Neural Information Processing Systems. 2019: 1586-1598.

Kwon G., Cai Z., Ravichandran A., et al. Masked Vision and Language Modeling for Multi-modal Representation Learning[EB/OL]. arXiv preprint arXiv:2208.02131, 2022.

## 1.3 基于大语言模型的VLMs

基于大语言模型的视觉语言模型利用已训练的大型语言模型或视觉特征提取器，学习文本与图像之间的映射关系。该方法的主要优势在于能够充分发挥预训练模型的丰富特征，从而显著减少从零开始训练所需的计算资源和数据量。其核心理念是通过有效结合视觉编码器与大型语言模型，避免了重新训练，从而实现对多模态数据的深刻理解。

Frozen是首批将大型语言模型应用于视觉语言任务的开创性模型之一。该模型通过一个轻量级映射网络，将视觉编码器连接至冻结状态的大语言模型。映射网络负责将视觉特征投影到文本标记嵌入空间。Frozen使用NF-ResNet-50作为视觉编码器，并与一个线性映射层连接，二者从头开始训练，而大语言模型（例如，在C4数据集上训练的、参数为7亿的Transformer）保持冻结状态，以保留其预先学习到的重要特征。在推理阶段，Frozen能够条件化生成文本，展示了其快速适应新任务、获取通用知识以及融合视觉与语言元素的能力。Frozen主要用于文本生成任务，如图像描述生成。在推理过程中，该模型接受图像和文本嵌入的输入，并生成与图像内容相关的文本描述。尽管Frozen的性能相对中等，但它为后续的开放式多模态零样本/少样本学习发展奠定了基础。

MiniGPT系列进一步扩展了这一概念，使得同时接收文本和图像输入并生成相应文本输出成为可能。其中，MiniGPT-4通过简单的线性投影层，将BLIP-2所用的图像编码器产生的图像嵌入与Vicuna语言模型的输入空间有效对接。由于这两个组件均为预训练，MiniGPT-4仅需针对线性投影层进行训练，训练过程分为两个阶段：第一阶段使用来自Conceptual Caption、SBU和LAION等数据集的五百万对数据；第二阶段进行400步的指令微调。通过这一过程，MiniGPT-4能够在短时间内，以有限的计算资源完成高效训练。MiniGPT-5则在此基础上增加了图像生成功能，使用生成标记创建新图像，这些标记被映射到特征向量并输入到冻结的图像生成模块中，进一步提升了多模态处理能力。

继MiniGPT系列创新后，Bai等人推出的Qwen系列模型，包括Qwen-VL和Qwen-VL-Chat，在多模态交互领域取得了显著进展。Qwen模型结合了大型语言模型和视觉编码器，增强了对视觉和语言信息的处理能力。在Qwen架构中，LLM以Qwen-7B为基础初始化，视觉编码器采用ViT-bigG。模型通过单层交叉注意力模块将视觉表示压缩成固定长度的序列，这些序列被输入到LLM中。这一设计提升了模型处理视觉和语言信息的效率，促进了多模态任务中更有效的信息交互，为多模态技术的发展提供了新的动力。

在多模态交互领域，Li等人提出的BLIP-2模型高效利用预训练模型，为图像到文本的转换提供了创新方案。BLIP-2通过冻结的预训练模型大幅缩短训练时间，使用如CLIP的视觉编码器生成图像嵌入，并将其映射到大型语言模型（如OPT）的输入空间。BLIP-2的关键组件是Q-Former，一个约含100-200M参数的Transformer，通过交叉注意力机制将随机初始化的“查询”向量与图像嵌入交互，并通过线性层将其投影到LLM输入空间。这种方法提高了训练效率，并在多模态任务中实现了更精细的特征对齐，显著提升了图像理解与文本生成的紧密联系。

参考文献：

Tsimpoukeelli M., Menick J., Cabi S., et al. Multimodal Few-Shot Learning with Frozen Language Models[C]. Conference on Neural Information Processing Systems. 2021: 200-212.

Zhu D., Chen J., Shen X., et al. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models[EB/OL]. arXiv preprint arXiv:2304.10592, 2023.

Brock A., De S., Smith S. L., et al. High-Performance Large-Scale Image Recognition Without Normalization[C]. International Conference on Machine Learning. 2021: 1059-1071.

Raffel C., Shazeer N., Roberts A., et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer[J]. Journal of Machine Learning Research, 2020, 21: 140:1-140:67.

Sharma P., Ding N., Goodman S., et al. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset for Automatic Image Captioning[C]. Meeting of the Association for Computational Linguistics. 2018: 2556-2565.

Gurevych I., Miyao Y. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset for Automatic Image Captioning[C]. Meeting of the Association for Computational Linguistics. 2018: 2556-2565.

Alayrac J. B., Donahue J., Luc P., et al. Flamingo: a Visual Language Model for Few-Shot Learning[C]. Conference on Neural Information Processing Systems. 2017: 5998-6008.

Li J., Li D., Savarese S., et al. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models[C]. International Conference on Machine Learning. 2023: 19730-19742.

**Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality.**

Ordnez V., Kulkarni G., Berg T. L. Im2Text: Describing Images Using 1 Million Captioned Photographs[C]. Conference on Neural Information Processing Systems. 2011: 1143-1151.

Schuhmann C., Vencu R., Beaumont R., et al. Laion-400m: Open Dataset of Clip-Filtered 400 Million Image-Text Pairs[EB/OL]. arXiv preprint arXiv:2111.02114, 2021.

Bai J., Bai S., Yang S., et al. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond[EB/OL]. arXiv preprint arXiv:2308.12966, 2023.

Bai J., Bai S., Chu Y., et al. Qwen Technical Report[EB/OL]. arXiv preprint arXiv:2309.16609, 2023.

## 1.4 生成式VLMs

生成式视觉语言模型是一类利用生成模型处理和理解视觉与语言信息的模型。这些模型能够生成图像或文本，或在给定一种模态的条件下生成另一种模态的内容。生成式模型的核心在于学习输入数据的分布，并基于该分布生成新的数据实例。在视觉语言模型的背景下，这意味着模型能够根据文本描述生成图像，或根据图像生成文本描述。

CoCa（Contrastive Captioner）是一种生成式文本生成器，结合了对比学习和生成损失来训练多模态文本解码器。CoCa接受图像编码器的输出和来自单模态文本解码器的文本嵌入，生成与图像内容相关的文本。CoCa在预训练阶段利用大规模的图像和文本数据集，通过将图像标签视为文本，学习图像和文本之间的关联。这使得CoCa不仅能生成图像描述，还能执行其他多模态理解任务，如视觉问答任务，而无需额外的适应性调整。

CM3leon是一个多模态生成模型，专注于文本到图像和图像到文本的生成任务。CM3leon借鉴了图像标记器和文本标记器的设计，将图像和文本编码为一系列标记，然后通过Transformer模型处理这些标记。CM3leon的训练分为两个阶段：首先是检索增强的预训练，使用基于CLIP的编码器作为检索器获取相关的多模态文档，并将其加入输入序列中；接着是监督式微调（Supervised Fine-Tuning, SFT），通过多任务指令调整模型，以处理并生成不同模态的内容。这一两阶段的训练方法使得CM3leon在多模态任务中表现优异，展示了自回归模型在处理文本和图像间复杂交互的能力。

Chameleon是一系列混合模态基础模型，能够生成并推理包含文本和图像内容的序列。Chameleon模型从一开始就设计为混合模态，使用统一架构处理所有模态——图像、文本和代码。这种集成方法采用基于标记的表示，将图像和文本转换为离散标记，使得相同的Transformer架构可以应用于图像和文本标记序列，而无需为每种模态单独设计编码器。早期的融合策略使得模型能够在不同模态之间无缝推理和生成，但也带来了优化稳定性和扩展性的技术挑战。

参考文献：

Yu J., Wang Z., Vasudevan V., et al. CoCa: Contrastive Captioners are Image-Text Foundation Models[EB/OL]. arXiv preprint arXiv:2205.01917, 2022.

Yu L., Shi B., Pasunuru R., et al. Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning[EB/OL]. arXiv preprint arXiv:2309.02591, 2023.

Gafni O., Polyak A., Ashual O., et al. Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors[C]. European Conference on Computer Vision. 2022: 89-106.

Zhang S., Roller S., Goyal N., et al. OPT: Open Pre-trained Transformer Language Models[EB/OL]. arXiv preprint arXiv:2205.01068, 2022.

Vaswani A., Shazeer N., Parmar N., et al. Attention is All you Need[C]. Advances in Neural Information Processing Systems. 2017: 5998-6008.

Team C. Chameleon: Chameleon: Mixed-Modal Early-Fusion Foundation Models[EB/OL]. arXiv preprint arXiv:2405.09818, 2024.

# 2 面向VLMs的对抗攻击方法

随着视觉语言模型的快速发展，对抗攻击的研究已经从单一的视觉模型扩展到了更为复杂的多模态模型。早期的研究主要集中于通过微小的输入扰动误导模型的视觉识别能力。然而，随着视觉语言模型在多模态理解和推理任务中表现出色，攻击者开始探索操纵图像和文本输入的新方法，并且设计出多样化的攻击效果。这些多模态攻击策略不仅增加了攻击的多样性，也提高了对模型鲁棒性的要求，推动了新的防御机制的发展。 

对抗攻击的定义是通过在输入的图像和文本中施加精心设计的扰动，使得模型生成攻击者指定的文本，例如错误的图像描述。这些攻击利用模型在处理多模态数据时的脆弱性，通过引入微小的、通常不易被人类察觉的扰动，诱导模型执行非预期的行为。因此，研究者需要关注这些攻击策略，并开发有效的防御措施，以确保视觉语言模型的安全性和可靠性。

在这一背景下，一系列针对视觉语言模型的对抗攻击研究应运而生。这些研究不仅揭示了多模态系统的潜在安全风险，还为构建更健壮的模型提供了宝贵的见解。例如，Carlini等人延续早期对抗样本生成的技术，通过最大化对抗图像扰动对模型输出有害文本的概率，指出大型语言模型和多模态模型在面对对抗攻击时可能违背设计原则，进而生成有害文本。Qi等人则关注视觉语言模型的越狱攻击，通过计算图像对抗扰动，绕过模型的安全防护，迫使模型执行本应被拒绝的有害指令。Bagdasaryan的研究强调了在图像特定区域嵌入对抗扰动的可能性，这种方法能在不显著改变图像语义内容的情况下，引导模型生成攻击者指定的文本。Schlarmann等人提出了非定向攻击，通过降低正确文本输出的概率来计算对抗扰动。然而，由于图像的正确描述可能有无穷多种，尽管对抗扰动能够避免某一正确描述，模型仍可能生成其他符合图像的正确描述，这在一定程度上削弱了攻击效果。Bailey等人则提出了“图像劫持”的概念，介绍了一种新的行为匹配算法，用于训练图像劫持，并展示了如何利用这一技术实施多种攻击，包括特定字符串攻击、上下文泄露攻击、越狱攻击和“幻觉”攻击。

对抗样本的存在引起了人们对机器学习系统稳健性和可靠性的广泛关注。尤其引人注目的是其迁移性，即一个模型上生成的对抗样本能够误导结构不同的其他模型。这一特性大大降低了攻击者的攻击难度，因为他们无需了解目标模型的具体架构和参数。更令人关注的是，学者们发现，对抗扰动即使被应用到不同的图像上，依然能够成功误导模型，表明对抗扰动具有一定的通用性，能够跨越图像内容发挥作用。这种跨图像迁移性进一步扩大了对抗样本的影响范围及潜在威胁。此外，研究还发现，针对特定任务生成的图像对抗扰动，在其他任务中也能产生影响。这意味着，对抗样本不仅能够在同一任务的不同模型间迁移，还能跨任务挑战模型的泛化能力。本文将介绍关于视觉语言模型中对抗样本在跨模型、跨提示和跨数据迁移性方面的研究进展。

参考文献：

Szegedy C., Zaremba W., Sutskever I., et al. Intriguing Properties of Neural Networks[EB/OL]. arXiv preprint arXiv:1312.6199, 2013.

Goodfellow I. J., Shlens J., Szegedy C.. Explaining and Harnessing Adversarial Examples[EB/OL]. arXiv preprint arXiv:1412.6572, 2014.

（VLMs主流的几种对抗攻击方法）

Carlini N., Nasr M., Choquette-Choo C. A., et al. Are Aligned Neural Networks Adversarially Aligned?[C]. Conference on Neural Information Processing Systems. 2023: 61478-61500.

Qi X., Huang K., Panda A., et al. Visual Adversarial Examples Jailbreak Aligned Large Language Models[C]. Conference on Innovative Applications of Artificial Intelligence. 2024: 21527-21536.

Bagdasaryan E., Hsieh T. Y., Nassi B., et al. Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs[EB/OL]. arXiv preprint arXiv:2307.10490, 2023.

Schilarmann C., Hein M. On the Adversarial Robustness of Multi-Modal Foundation Models[C]. IEEE/ CVF International Conference on Computer Vision. 2023: 3697-3687.

Bailey L., Ong E., Russell S., et al. Image Hijacks: Adversarial Images can Control Generative Models at Runtime[EB/OL]. arXiv preprint arXiv:2309.00236, 2023.

（介绍对抗样本具有迁移性的现象）

Gu J., Jia X., Jorge P. A Survey on Transferability of Adversarial Examples Across Deep Neural Networks[EB/OL]. arXiv preprint arXiv:2310.17626, 2023.

Yu W., Gu J., Li Z., et al. Reliable Evaluation of Adversarial Transferability[EB/OL]. arXiv preprint arXiv:2306.08565, 2023.

（介绍对抗样本具有迁移性的类别）

Naseer M., Khan S. H., Rahman S., et al. Task-Generalizable Adversarial Attack Based on Perceptual Metric[EB/OL]. arXiv preprint arXiv:1811.09020, 2018.

Naseer M., Khan S. H., Khan H. M., et al. Cross-Domain Transferability of Adversarial Perturbations[C]. Conference on Neural Information Processing Systems. 2019: 12885-12895.

Lu Y., Jia Y., Wang J., et al. Enhancing Cross-Task Black-Box Transferability of Adversarial Examples with Dispersion Reduction[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 937-946.

Nakka K. K., Salzmann M. Learning Transferable Adversarial Perturbations[C]. Conference on Neural Information Processing Systems. 2021: 13950-13962.

## 2.1 跨模型迁移性增强方法

对抗样本的跨模型迁移性是指同一个对抗样本能够成功地欺骗不同结构或训练数据不同的模型。对此现象的原因，学术界已有多种解释。Nguyen等人提出，不同模型在训练过程中可能会捕捉到相似的图像特征，这为对抗样本的迁移提供了可能性。Goodfellow等则从模型权重的角度分析，认为由于训练数据的相似性，不同模型的权重趋向于一致，从而促进了对抗样本的迁移。此外，Dong等学者强调，模型的决策边界是对抗样本生成的关键因素。

虽然对抗样本的跨模型迁移性已经有了一些理论解释，但如何让对抗样本在不同模型间更加有效地“迁移”仍然是一个研究热点。为了增强这一效果，研究者们提出了多种方法，旨在提高对抗样本在不同模型上的适应性。接下来，我们将介绍一些具体的技术和策略，这些方法能够帮助对抗样本更好地跨模型传播。

针对早期的视觉模型，Xie等人首次提出通过对原始图像进行可微分变换以增强对抗样本的迁移性。在生成对抗扰动的每一步中，他们引入了随机图像变换操作，例如按一定概率对图像进行大小调整和填充。实验表明，随着变换概率的增加，对抗样本的跨模型迁移能力显著提高。除了数据增强的策略外，Dong等人进一步优化了对抗样本的生成方法，通过将动量机制引入快速梯度符号法（FGSM），提出了改进算法 MI-FGSM（Momentum Iterative Fast Gradient Method）。这一方法利用动量累计历史梯度信息，使对抗扰动的更新更加稳定，从而提升了跨模型的迁移效果。此外，Li 等人针对可迁移对抗样本的两个关键问题进行了研究：一是迭代攻击中梯度幅值逐渐减小，导致动量累积时连续两次扰动过于相似，即“噪声固化”问题；二是对抗样本需要同时逼近目标类别并远离真实类别的矛盾性。为此，他们首次引入庞加莱球作为度量空间，解决了噪声固化问题，使梯度幅值能够自适应调整，并提升了噪声方向的灵活性。同时，他们设计了一种基于庞加莱距离的损失函数，以替代传统交叉熵损失，仅在逼近目标类别时施加梯度更新，从而进一步增强对抗攻击的迁移效果。

在视觉语言模型领域，对抗样本的跨模型迁移性同样备受关注。由于这类模型不仅处理视觉特征，还需结合语言信息，其复杂性使得增强跨模型迁移性的方法更具挑战性。针对这一问题，研究者们提出了一系列针对视觉语言模型的优化策略，试图通过融合多模态特征和改进生成机制，提升对抗样本在视觉语言模型间的迁移效果。在集成模型方向，Guo等人提出的 AdvDiffVLM 方法使用自适应集成梯度估计（Adaptive Ensemble Gradient Estimation）模块。该模块通过多个代理模型估计目标模型的梯度信息，从而在扩散模型生成对抗图像的过程中有效嵌入对抗语义。Niu等人设计了一种基于最大似然的算法，用于生成图像越狱提示以攻击视觉语言模型。他们采用了多个代理模型，包括基于 Vicuna-7B、Vicuna-13B 和 LLaMA-2-7B 的 MiniGPT-4。Wu等人则通过整合 ViT-B/32、ViT-B/16、ViT-L/14 和 ViT-L/14@336px 等模型，对多模态智能体进行幻觉攻击与目标误导攻击。Dong等人进一步提出一种方法，在生成图像扰动时以 ViT-B/16、CLIP 和 BLIP-2 作为代理模型，并结合 SSA-CWA 进行攻击，以提升对抗样本的迁移性。此外，部分研究将图像嵌入作为提升跨模型迁移性的切入点。例如，Dong等人提出一种图像嵌入攻击方法，旨在通过增加对抗图像与原始图像嵌入之间的差异来误导模型。针对可能导致模型输出其他正确描述的风险，Zhao等人提出结合扩散模型将目标文本转换为图像，并通过拉近对抗图像与生成图像嵌入的相似度来进行优化。在文本描述攻击方面，Zhao等人提出采用随机梯度无关（RGF, Random Gradient-Free）方法来估计梯度，成功攻击了未见过的视觉语言模型。

从模型对齐的角度，Ma 等人提出了一种微调源模型（Source Model）的策略，使其输出与一组独立的见证模型（Witness Models）的输出接近，然后利用源模型生成对抗样本，从而提升跨模型迁移能力。Lu 等人提出了 SGA（Set-level Guidance Attack） 方法，通过结合跨模态交互和对齐保持的数据增强技术，生成可在黑盒环境中高效攻击多个模型的对抗样本。Han 等人则从最优传输（Optimal Transport, OT）理论出发，将图像和文本集合的特征视为两个分布，通过最优传输计算两者间的最优映射关系，有效缓解过拟合问题，并提升对抗样本的迁移性。

参考文献：

（介绍对抗样本具有迁移性的原因）

Nguyen A. M., Yosinski J., Clune J. Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images[C]. IEEE Conference on Computer Vision and Pattern Recognition. 2015: 427-436.

Goodfellow I. J., Shlens J., Szegedy C.. Explaining and Harnessing Adversarial Examples[EB/OL]. arXiv preprint arXiv:1412.6572, 2014.

Dong Y., Liao F., Pang T., et al. Boosting Adversarial Attacks with Momentum[C]. IEEE Conference on Computer Vision and Pattern Recognition. 2018: 9185-9193.

（传统的跨模型迁移性增强）

-数据增强

Xie C., Zhang Z., Zhou Y., et al. Improving Transferability of Adversarial Examples With Input Diversity[C]. Conference on Computer Vision and Pattern Recognition. 2019: 2730-2739.

-优化方法

Dong Y., Liao F., Pang T., et al. Boosting Adversarial Attacks with Momentum[C]. IEEE Conference on Computer Vision and Pattern Recognition. 2018: 9185-9193.

-损失函数

Li M., Deng C., Li T., et al. Towards Transferable Targeted Attack[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 638-646.

（VLMs的跨模型迁移性增强）

Guo Q., Pang S., Jia X., et al. Efficiently Adversarial Examples Generation for Visual-Language Models under Targeted Transfer Scenarios using Diffusion Models[EB/OL]. arXiv preprint arXiv:2404.10335, 2024.

Niu Z., Ren H., Gao X., et al. Jailbreaking Attack Against Multimodal Large Language Model[EB/OL]. arXiv preprint arXiv:2402.02309, 2024.

Wu C. H., Koh J. Y., Salakhutdinov R., et al. Adversarial Attacks on Multimodal Agents[EB/OL]. arXiv preprint arXiv:2406.12814, 2024.

Dong Y., Chen H., Chen J., et al. How Robust is Google's Bard to Adversarial Image Attacks?[EB/OL]. arXiv preprint arXiv:2309.11751, 2023.

Zhao Y., Pang T., Du C., et al. On Evaluating Adversarial Robustness of Large Vision-Language Models[C]. Conference on Neural Information Processing Systems. 2023: 54111-54138.

-其他

Ma A., Farahmand A., Pan Y., et al. Improving Adversarial Transferability via Model Alignment[C]. European Conference on Computer Vision. 2024: 74-92.

Lu D., Wang Z., Wang T., et al. Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models[C]. IEEE/CVF International Conference on Computer Vision. 2023: 102-111.

Han D., Jia X., Bai Y., et al. OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization[EB/OL]. arXiv preprint arXiv:2312.04403, 2023.

## 2.2 跨提示迁移性增强方法

大纲：

介绍跨提示迁移性的概念

介绍跨任务迁移性和跨提示迁移性的区别；

介绍传统模型的跨任务迁移性的研究；

介绍方法；

内容：

视觉语言模型凭借其对文本提示的灵活适配能力，在无需架构调整的情况下即可完成多种视觉任务，例如图像分类、图像描述和视觉问答等。这种模型通过将视觉信息和文本提示进行联合建模，展现了强大的任务泛化能力。然而，并非所有任务都可以单纯依赖文本提示完成，例如视觉定位任务需要额外的架构设计和大量预训练支持。Minigpt-v2 和 Kosmos-2 等模型正是通过大规模预训练才具备了执行视觉定位的能力。因此，许多研究着重于对抗样本的跨提示迁移性进行研究。

参考文献：

（传统模型的跨任务迁移性）

（视觉定位）

Chen J., Zhu D., Shen X., et al. MiniGPT-v2: Large Language Model as A Unified Interface for Vision-Language Multi-Task Learning[EB/OL]. arXiv preprint arXiv:2310.09478, 2023.

Peng Z., Wang W., Dong L., et al. Kosmos-2: Grounding Multimodal Large Language Models to the World[EB/OL]. arXiv preprint arXiv:2306.14824, 2023.

（VLMs的跨提示迁移性）

Bailey L., Ong E., Russell S., et al. Image Hijacks: Adversarial Images can Control Generative Models at Runtime[EB/OL]. arXiv preprint arXiv:2309.00236, 2023.

Lu D., Pang T., Du C., et al. Test-Time Backdoor Attacks on Multimodal Large Language Models[EB/OL]. arXiv preprint arXiv:2402.08577, 2024.

Luo H., Gu J., Liu F., et al. An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models[C]. International Conference on Learning Representations. 2024: 1-22.

## 2.3 跨数据迁移性增强方法

大纲：

介绍跨跨数据迁移性的定义；

介绍当前视觉语言模型的跨数据迁移性分为几类；

介绍方法；

内容：



参考文献：

（传统的跨数据迁移性）

视觉语言模型：

图像

Lu D., Pang T., Du C., et al. Test-Time Backdoor Attacks on Multimodal Large Language Models[EB/OL]. arXiv preprint arXiv:2402.08577, 2024.

语料库

Qi X., Huang K., Panda A., et al. Visual Adversarial Examples Jailbreak Aligned Large Language Models[C]. Conference on Innovative Applications of Artificial Intelligence. 2024: 21527-21536.

Wang R., Ma X., Zhou H., et al. White-Box Multimodal Jailbreaks Against Large Vision-Language Models[C]. Conference on Multimedia. 2024: 6920-6928.

Ying Z., Liu A., Zhang T., et al. Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt[EB/OL]. arXiv preprint arXiv:2406.04031, 2024.

综述：

Bordes F., Pang R. Y., Ajay A., et al. An Introduction to Vision-Language Modeling[EB/OL]. arXiv preprint arXiv:2405.17247, 2024.

Zhang C., Xu X., Wu J., et al. Adversarial Attacks of Vision Tasks in the Past 10 Years: A Survey[EB/OL]. arXiv preprint arXiv:2410.23687, 2024.

# 3 现存问题与发展趋势



# 4 结论

