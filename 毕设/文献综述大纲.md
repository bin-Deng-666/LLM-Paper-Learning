请你充当一名论文编辑专家，在论文评审的角度去修改论文，使其更加流畅，优美。下面是具体要求：  

1.能让读者快速获得文章的要点或精髓，让文章引人入胜；能让读者了解全文中的重要信息、分析和论点；帮助读者记住论文的要点 

 2.用简洁、明了的语言描述您的方法和结果，以便评审更容易理解论文  

下文是论文的一部分，请你修改它：

# 1  视觉语言模型的研究现状

大纲：

介绍视觉语言模型；介绍视觉语言模型基于Transformer的初步工作；给出视觉语言模型的四种分类；给出各种分类的定义；

内容：

视觉语言模型（Visual-Language Models, VLMs）通过将计算机视觉与自然语言处理相结合，旨在实现图像与文本的多模态理解。这些模型在图像描述生成、视觉问答（Visual Question Answering, VQA）和视觉定位（Visual Grounding, VG）等跨模态任务中展现了巨大的潜力。近年来，得益于Transformer架构在自然语言处理和计算机视觉领域的广泛应用，基于Transformer的视觉语言模型成为多模态任务的研究主流，并在相关领域取得了显著进展。

在视觉语言模型的早期探索中，受BERT（Bidirectional Encoder Representations from Transformers）在自然语言处理领域的卓越表现启发，许多研究者尝试将其扩展到视觉任务中。模型如Visual-BERT和ViLBERT首次将Transformer架构应用于视觉语言任务，结合图像与文本信息，在掩码语言建模（Masked Language Modeling）和图像-文本匹配（Image-Text Matching）任务中取得了优异的成绩。通过Transformer的自注意力机制，这些模型能够有效学习词汇与视觉特征之间的关联，为后续研究奠定了坚实基础。

随着视觉语言模型需求的不断增加，早期的研究逐渐发展出四种主流架构类型。第一类是对比学习模型，利用正负样本对比，通过将相似的图像-文本对映射到相近的嵌入空间，提升跨模态对齐效果。第二类是掩码模型，通过部分遮掩输入信息，促使模型在恢复缺失内容的过程中进行推测和重建。第三类是基于预训练骨干的模型，通常借助大型预训练语言模型（如Llama）作为基础，通过图像编码器与语言模型的对齐，实现更精准的图像文本匹配。第四类是生成式模型，这些模型能够直接生成图像或文本内容，满足多样化任务需求。这四种主流架构为视觉语言模型的发展提供了多种实现方式，从高效对齐到多样化生成，为多模态任务奠定了坚实的技术基础。

值得注意的是，这些视觉语言模型架构并非孤立存在，许多模型在设计时结合了对比学习、掩码训练和生成策略。接下来，我们将在每种架构中介绍一两个代表性模型，展示其设计思路。

参考文献：

Attention is all you need

BERT: Pre-training of deep bidirectional transformers for language understanding

VisualBERT: A simple and performant baseline for vision and language

Vilbert: Pretraining task-agnostic vision-linguistic representations for vision-and-language tasks

Llama: Open and efficient foundation language models

## 1.1 对比学习模型

大纲：

介绍contrastive视觉语言模型的定义；介绍Clip、SigLIP、Llip；

内容：

基于对比学习的视觉语言模型旨在学习图像与文本之间的对齐表示，核心思想是利用对比损失函数优化模型参数，从而实现跨模态的语义关联。在这种方法中，正样本对由一张图像及其对应的真实文本描述组成，负样本对则是将同一张图像与描述其他图像的文本配对。通过区分正样本对与负样本对，这些视觉语言模型能够将语义相关的图像和文本映射到嵌入空间中的相近位置，而不相关的则被推向较远的位置。

CLIP（Contrastive Language-Image Pre-Training）是这一领域的代表性模型。CLIP通过对比学习框架训练视觉和文本编码器，使得图像和其对应的文本描述在嵌入空间中具有相似的向量表示。CLIP的训练数据集包含从网络上收集的四亿图像和文本对。依靠这些数据，CLIP能够学习到丰富的视觉和语言特征，并在零样本分类等任务中展现出非常卓越的性能。具体来说，基于ResNet-101的CLIP模型在零样本分类准确率上达到了76.2%，并在多个鲁棒性基准测试中超过了经过监督训练的ResNet模型。

SigLIP（Signature Language Image Pre-training）是一种基于CLIP改进的对比学习方法，主要区别在于损失函数的选择。与CLIP使用信息噪声对比估计（Information Noise Contrastive Estimation，InfoNCE）损失函数不同，SigLIP采用了基于二元交叉熵的噪声对比估计（Noise Contrastive Estimation，NCE）损失函数。这一损失函数的调整使得SigLIP在较小批量数据的情况下仍能展现出优异的零样本性能，从而减少了对大规模数据的依赖。

Llip（Latent Language Image Pretraining）模型进一步改进了图像与文本之间的关联方式。考虑到一张图像可以用多种不同的方式进行描述，Llip提出了一种新的条件编码机制，通过交叉注意力模块来根据目标描述调整图像的编码。这种方法不仅提高了编码的多样性，还增强了表示的表达能力，从而在零样本分类和检索任务中实现了性能提升。Llip通过动态调整图像特征的表示，使得模型能够更好地捕捉图像与文本之间复杂的语义关系，进一步提升了跨模态学习的效果。

参考文献：

Learning transferable visual models from natural language supervision.

Deep residual learning for image recognition.

Sigmoid loss for language image pre-training.

Modeling caption diversity in contrastive vision-language pretraining. 

NCE：Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.

InfoNCE：Representation learning with contrastive predictive coding.

## 1.2 掩码模型

大纲：

介绍masking视觉语言模型的定义；介绍Flava、MaskVLM；

内容：

在大语言模型的早期研究中，BERT通过掩码语言建模（Masked Language Modeling, MLM）技术，预测句子中缺失的词元，从而显著提升了在多种自然语言处理任务中的表现。受到BERT文本掩码技术的启发，视觉语言模型领域也开始采用类似的掩码策略，尤其是在图像编码方面。例如，MAE和I-JEPA便是通过掩码图像建模（Masked Image Modeling, MIM）技术来训练图像编码器。这些基于掩码的视觉语言模型的核心思想是，通过随机去除图像输入的部分区域，迫使模型在缺失信息的情况下进行推理，从而提升其跨模态信息的理解能力。

FLAVA（Foundational Language and Vision Alignment）是基于掩码方法的典型模型。其架构包括三个核心组件，均基于Transformer框架并针对不同模态进行优化。图像编码器使用ViT（Vision Transformer）将图像转换为图像块，通过线性嵌入和Transformer表示进行处理，并附带一个分类标记（[CLSI]）。文本编码器则使用Transformer对文本进行标记化，并将其嵌入为向量进行上下文处理，输出隐藏状态向量，并附带一个分类标记（[CLST]）。这两个编码器都采用掩码训练方法。多模态编码器融合图像和文本编码器的隐藏状态，利用线性投影和跨模态注意力机制整合视觉与文本信息，并引入额外的多模态分类标记（[CLSM]）。FLAVA模型通过结合多模态和单模态的掩码建模损失，并辅以对比学习目标，展示了卓越的多功能性和有效性。在7000万对公开图像和文本的数据集上进行预训练后，FLAVA在35个涵盖视觉、语言和多模态基准的任务中取得了最先进的性能，展现了其强大的跨领域信息理解与整合能力。

FLAVA模型的局限性在于它依赖于预训练的视觉编码器，例如dVAE。为了解决这一问题，Kwon等人提出了MaskVLM。与FLAVA不同，MaskVLM不依赖于预训练的视觉编码器，而是直接在像素空间和文本标记空间上应用掩码策略。MaskVLM利用信息流动的方式，使得一个模态中的信息能够有效地传递到另一个模态，从而使模型能够在两个模态之间进行有效的联合学习。举例来说，在文本重构任务中，模型会利用来自图像编码器的信息来重构文本内容；而在图像任务中，模型也可以利用来自文本编码器的信息来辅助图像的处理和理解。这种互通的信息流动机制是MaskVLM能够跨越视觉和语言两种模态，减少对第三方模型依赖的关键因素。

参考文献：

BERT: Pre-training of deep bidirectional transformers for language understanding

Masked autoencoders are scalable vision learners.

Self-supervised learning from images with a joint-embedding predictive architecture.

Flava: A foundational language and vision alignment model.

An image is worth 16x16 words: Transformers for image recognition at scale.

D-VAE: Avariational autoencoder for directed acyclic graphs.

Masked vision and language modeling for multi-modal representation learning.

## 1.3 基于预训练骨干的模型

大纲：

介绍基于pretrained backbones视觉语言模型的定义；介绍Frozen、MiniGPT；稍微介绍qwen和blip-2；

内容：



参考文献：

Multimodal few-shot learning with frozen language models.

High-performance large-scale image recognition without normalization.

Exploring the limits of transfer learning with a unified text-to-text transformer.

Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.

Flamingo: a visual language model for few-shot learning.

MiniGPT-4: Enhancing vision-language understanding with advanced large language models.

BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality, March 2023.

Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.

Im2text: Describing images using 1 million captioned photographs.

Laion-400m: Open dataset of CLIP-filtered 400 million image-text pairs.

Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.

Qwen technical report.

## 1.4 生成式模型

大纲：

介绍Generative-based视觉语言模型的定义；介绍CoCa、CM3leon和Chameleon；

内容：



参考文献：

Coca: Contrastive captioners are image-text foundation models.

Scaling autoregressive multi-modal models: Pretraining and instruction tuning.

Make-a-scene: Scene-based text-to-image generation with human priors.

OPT: Open pre-trained transformer language models.

Attention is all you need

Mixed-modal early-fusion foundation models.

# 2 面向视觉语言模型的对抗攻击方法设计

## 2.1 对抗攻击的早期研究

给出对抗攻击的定义；介绍早期对抗攻击的逐步发展，主要介绍FGSM及其变种；

参考文献：

Intriguing Properties of Neural Networks

Explaining and Harnessing Adversarial Examples

Adversarial Examples in the Physical World

Boosting Adversarial Attacks With Momentum

Improving Transferability of Adversarial Examples With Input Diversity

Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks

Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks

Towards Deep Learning Models Resistant to Adversarial Attacks

## 2.2 面向视觉语言模型的对抗攻击策略

介绍随着发展对抗攻击拓展到了视觉语言模型；定义视觉语言模型上对抗攻击并解释它的多样性；介绍基础VLM对抗攻击设计方法；

参考文献：

Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs

Are Aligned Neural Networks Adversarially Aligned?

How Robust is Google’s Bard to Adversarial Image Attacks?

On Evaluating Adversarial Robustness of VLMs

On the Adversarial Robustness of Multi-Modal Foundation Models

Visual Adversarial Examples Jailbreak Aligned LLMs

An Image is Worth 1000 Lies

Image Hijacks

Adversarial Robustness for Visual Grounding of MLLMs

# 3 面向视觉语言模型的对抗样本迁移性研究

## 3.1 对抗样本迁移性的基础理论

初步概述对抗攻击并引出迁移性；强调对抗样本迁移性的研究重要性；介绍data augmentation、optimization technique、loss objective、model component四类迁移性攻击；

参考文献：

Delving into transferable adversarial examples and black-box attacks.

Improving transferability of adversarial examples with input diversity

Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks

Boosting adversarial attacks with momentum.

A method for unconstrained convex minimization problem with the rate of convergence o(1/kˆ 2).

Towards transferable targeted attack.

Improving transferability of adversarial patches on face recognition with generative models.

Transferable adversarial perturbations.

Task-generalizable adversarial attack based on perceptual metric.

## 3.2 面向视觉语言模型的对抗样本迁移性探索

介绍从视觉模型发展到视觉语言模型对抗样本迁移的重要性；介绍迁移性的主体：模型和提示；

参考文献：

Transferable multimodal attack on vision-language pre-training models. 

How Robust is Google’s Bard to Adversarial Image Attacks?

On Evaluating Adversarial Robustness of VLMs

An Image is Worth 1000 Lies

Adversarial Robustness for Visual Grounding of MLLMs

Visual Adversarial Examples Jailbreak Aligned LLMs

# 4 现存问题与发展趋势



# 5 结论

