请您充当论文编辑专家，站在论文评审的角度，对论文进行修改，使其更加流畅、优美，并提升可读性。具体要求如下：

1. 引人入胜并突出要点：通过修改，使文章能够迅速吸引读者的注意力，并帮助读者快速抓住文章的核心要点、分析和论点。确保文章内容简洁明了，重点突出，让读者在阅读过程中能够清楚理解文章的核心信息。
2. 简洁的描述方法与结果：用简洁而清晰的语言对论文中的方法和结果进行详细描述，确保评审能够轻松理解论文的研究过程和结论。描述应避免含糊不清，尽量精确地表达技术细节和实验数据，帮助评审准确理解论文的贡献。
3. 优化语言流畅性与逻辑结构：改善论文的语言流畅性，使文章的结构更为清晰、连贯。避免复杂冗长的句子和结构，使得每一部分内容都能自然地过渡到下一部分，提升论文的整体可读性。
4. 避免重复与无关内容：删除不必要的重复和冗余部分，确保每段内容都具有清晰的目的，并与论文的主题紧密相关。

下文是论文的一部分，请你修改它：

# 1  视觉语言模型的研究现状

视觉语言模型（Visual-Language Models, VLMs）结合了计算机视觉与自然语言处理技术，旨在实现图像与文本的多模态信息理解。它们在图像描述生成（Image Captioning, IC）、视觉问答（Visual Question Answering, VQA）和视觉定位（Visual Grounding, VG）等跨模态任务中展现了巨大的潜力。近年来，Transformer架构在自然语言处理和计算机视觉领域的广泛应用，为视觉语言模型的发展提供了强大支持。得益于Transformer的自注意力机制，模型能够同时处理图像与文本数据，为多模态信息理解提供了创新解决方案。因此，基于Transformer的视觉语言模型已逐渐成为多模态任务研究的主流，并在多个应用领域取得了显著进展。

视觉语言模型的早期发展受到BERT（Bidirectional Encoder Representations from Transformers）在自然语言处理领域成功的启发。随着BERT及其变种模型的广泛应用，许多研究者开始将其架构扩展到图像与文本结合的跨模态任务中。Visual-BERT和ViLBERT是这一探索的代表性模型，它们首次将Transformer架构应用于视觉语言任务，尤其在掩码语言建模（Masked Language Modeling）和图像-文本匹配（Image-Text Matching）任务中取得了显著成果。这些模型通过Transformer的自注意力机制，成功捕捉图像与文本之间的相互关系，促进了模型对多模态信息的理解。这些开创性工作为后续研究提供了宝贵的经验，并为进一步的发展奠定了坚实的基础。

随着视觉语言模型应用需求的不断增长，研究者提出了四种主流架构类型。第一类是基于对比学习的模型，这类模型通过对正负样本的对比，推动图像-文本对的嵌入空间对齐，从而显著提高跨模态对齐效果。第二类是基于掩码图像建模的模型，这类模型通过部分遮掩输入的图像或文本信息，迫使模型在恢复缺失内容时进行推测和重建，提升了推理能力。第三类是基于大语言模型构建的模型，这类模型通常采用大型预训练语言模型（如Llama等）作为骨干，结合图像编码器与语言模型对齐，显著提高图像-文本匹配的准确度，并在多个任务中展现出优异性能。第四类是生成式模型，这些模型不仅能直接生成图像或文本，广泛应用于图像描述和生成等任务，还能为创新应用提供解决方案，如图像生成与编辑。

这四种主流架构为视觉语言模型的发展提供了多种可行的技术路径，从高效的跨模态对齐到创新的图像生成，极大地推动了视觉语言模型的应用与技术进步。值得注意的是，这些架构并非孤立存在，许多模型在设计时巧妙地融合了对比学习、掩码训练与生成策略的优点。接下来，我们将深入探讨每种架构中的一到两个代表性模型，并详细分析其设计思路与应用场景。

参考文献：

Vaswani A., Shazeer N., Parmar N., et al. Attention is All you Need[C]. Conference on Neural Information Processing Systems. 2017: 5998-6008.

Devlin J., Chang M. W., Lee K., et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2019: 4171-4186.

Li L. H., Yatskar M., Yin D., et al. Visualbert: A Simple and Performant Baseline for Vision and Language[EB/OL]. arXiv preprint arXiv:1908.03557, 2019.

Lu J., Batra D., Parikh D., et al. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks[C]. Conference on Neural Information Processing Systems. 2019: 13-23.

Touvron H., Lavril T., Izacard G., et al. Llama: Open and Efficient Foundation Language Models[EB/OL]. arXiv preprint arXiv:2302.13971, 2023.

## 1.1 基于对比学习的VLMs

基于对比学习的视觉语言模型旨在学习图像与文本之间的对齐表示，其核心思想是通过对比损失函数优化模型参数，促使图像和文本在嵌入空间中实现跨模态的语义关联。具体而言，这类模型将正样本对（由图像及其对应的真实文本描述组成）与负样本对（将同一张图像与描述其他图像的文本配对）进行对比学习。通过区分正负样本对，这些视觉语言模型能够将语义相关的图像和文本映射到嵌入空间中的相近位置，而不相关的样本则被推向较远的位置。这种方法有助于提高跨模态对齐的精度，使模型能够更好地理解图像与文本之间的关系。

CLIP（Contrastive Language-Image Pre-Training）是这一领域的代表性模型。CLIP利用对比学习框架训练视觉和文本编码器，使图像和其对应的文本描述在嵌入空间中具有相似的向量表示。CLIP的训练数据集包含了四亿对来自网络的图像和文本。这些大规模的数据使得CLIP能够学习到丰富的视觉和语言特征，并在多个任务中展现出强大的性能，尤其是在零样本分类任务中表现卓越。以基于ResNet-101的CLIP模型为例，它在零样本分类准确率上达到了76.2%，并且在多个鲁棒性基准测试中，CLIP模型超越了经过监督训练的ResNet模型。

SigLIP（Signature Language Image Pre-training）是一种基于CLIP的改进模型，其主要创新体现在损失函数的设计上。与CLIP使用的信息噪声对比估计（Information Noise Contrastive Estimation，InfoNCE）损失函数不同，SigLIP采用了基于二元交叉熵的噪声对比估计（Noise Contrastive Estimation，NCE）损失函数。通过这一调整，SigLIP能够在较小批量数据下仍然展现出优异的零样本性能，从而减少了对大规模数据的依赖。

Llip（Latent Language Image Pretraining）模型进一步改进了图像与文本之间的关联方式。考虑到一张图像可以用多种不同的方式进行描述，Llip提出了一种新的条件编码机制，通过交叉注意力模块来根据目标描述调整图像的编码。这种方法的核心在于通过动态调整图像编码，使得每个图像可以针对不同的描述生成不同的编码表示，从而提高了编码的多样性和表达能力。Llip通过这种条件编码机制，能够灵活地适应各种任务需求，并进一步推动了跨模态学习的进展。

参考文献：

Radford A., Kim W. J., Hallacy C., et al. Learning Transferable Visual Models From Natural Language Supervision[C]. International Conference on Machine Learning. 2021: 8748-8763.

He K., Zhang X., Ren S., et al. Deep Residual Learning for Image Recognition[C]. IEEE Conference on Computer Vision and Pattern Recognition. 2016: 770-768.

Zhai X., Mustafa B., Kolesnikov B., et al. Sigmoid Loss for Language Image Pre-Training[C]. IEEE/CVF International Conference on Computer Vision. 2023: 11941-11952.

Lavoie S., Kirichenko P., Ibrahim M., et al. Modeling Caption Diversity in Contrastive Vision-Language Pretraining. International Conference on Machine Learning. 2024: 26070-26084.

Gutmann M., Hyvärinen A. Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models[C]. International Conference on Artificial Intelligence and Statistics. 2010: 297-304.

Oord A., Li Y., Vinyals O. Representation Learning with Contrastive Predictive Coding[EB/OL]. arXiv preprint arXiv:1807.03748, 2018.

## 1.2 基于掩码图像建模的VLMs

在大语言模型的早期研究中，BERT通过掩码语言建模（Masked Language Modeling, MLM）技术，预测句子中缺失的词元，从而显著提升了在多种自然语言处理任务中的表现。受到BERT文本掩码技术的启发，视觉语言模型领域也开始采用类似的掩码策略，特别是在图像编码方面。例如，MAE（Masked Autoencoders）和I-JEPA（Image-Joint Encoding and Pretraining Architecture）便是通过掩码图像建模（Masked Image Modeling, MIM）技术来训练图像编码器。这些基于掩码的视觉语言模型的核心思想是，通过随机去除图像输入的部分区域，迫使模型在缺失信息的情况下进行推理，从而提升其跨模态信息的理解能力。

FLAVA（Foundational Language and Vision Alignment）是基于掩码方法的典型模型。该模型架构包括三个核心组件，均基于Transformer框架，并针对不同模态进行了优化。具体而言，图像编码器采用ViT（Vision Transformer）将图像分割为图像块，并通过线性嵌入和Transformer表示进行处理，处理过程中还会附带一个分类标记（[CLSI]）。文本编码器则使用Transformer对文本进行标记化，将文本嵌入为向量并进行上下文处理，输出隐藏状态向量，并附带一个分类标记（[CLST]）。这两个编码器都采用了掩码训练方法。多模态编码器则融合了图像和文本编码器的隐藏状态，借助线性投影和跨模态注意力机制有效整合视觉与文本信息，并引入额外的多模态分类标记（[CLSM]）。FLAVA模型通过结合多模态和单模态的掩码建模损失，并辅以对比学习目标，展现了卓越的多功能性和有效性。在7000万对公开图像和文本数据上进行预训练后，FLAVA在35个涵盖视觉、语言和多模态的基准任务中取得了最先进的性能，展示了其强大的跨领域信息理解与整合能力。

尽管FLAVA模型在多模态任务中表现出色，但其局限性在于依赖于预训练的视觉编码器（例如dVAE）。为了解决这一问题，Kwon等人提出了MaskVLM（Masked Vision-Language Model）。与FLAVA不同，MaskVLM不依赖于预训练的视觉编码器，而是直接在像素空间和文本标记空间上应用掩码策略。这一创新使得MaskVLM能够在没有外部视觉编码器的帮助下，直接处理图像和文本输入。MaskVLM通过信息流动机制，允许一个模态中的信息有效传递到另一个模态，从而使得模型能够在视觉和语言两个模态之间进行有效的联合学习。例如，在文本重构任务中，MaskVLM利用图像编码器的信息来辅助重构文本内容；而在图像任务中，模型则可以利用文本编码器提供的信息来辅助图像的处理和理解。

参考文献：

Devlin J., Chang M. W., Lee K., et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[C]. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2019: 4171-4186.

He K., Chen X., Xie S., et al. Masked Autoencoders are Scalable Vision Learners[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 15979-15988.

Assran M., Duval Q., Misra I., et al. Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 15619-15629.

Singh A., Hu R., Goswami V., et al. Flava: A Foundational Language and Vision Alignment Model[C]. IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 15617-15629.

**Dosvitskiy A., Beyer L., Kolesnikov A., et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale[C]. International Conference on Learning Representations. 2021: ??????-???????.**

Zhang M., Jiang S., Cui Z., et al. D-VAE: A Variational Autoencoder for Directed Acyclic Graphs[C]. Conference on Neural Information Processing Systems. 2019: 1586-1598.

**Kwon G., Cai Z., Ravichandran A., et al. Masked Vision and Language Modeling for Multi-modal Representation Learning[C]. International Conference on Learning Representations. 2023: ??????-???????.**

## 1.3 基于大语言模型的VLMs

基于大语言模型的视觉语言模型利用已训练的大型语言模型或视觉特征提取器，学习文本与图像之间的映射关系。这一方法的主要优势在于能够充分发挥预训练模型所积累的丰富特征，从而显著减少从零开始训练所需的大量计算资源和数据。其核心理念是通过有效结合视觉编码器与大型语言模型，避免了完全重新训练，以实现对多模态数据的深刻理解。

Frozen是首批将大型语言模型应用于视觉语言任务的开创性模型之一。它通过一个轻量级映射网络，将视觉编码器连接至冻结状态下的大语言模型。该映射网络负责将视觉特征投影到文本标记嵌入空间。在Frozen中，使用NF-ResNet-50作为视觉编码器，并连接线性映射层，二者均为从头开始进行训练，而大语言模型（例如，在C4数据集上经过培训、参数达7亿的Transformer）则保持冻结，以保留其预先学习到的重要特征。在推理阶段，Frozen能够条件化地生成文本，这展示了其快速适应新任务、获取通用知识以及融合视觉与语言元素的能力。

Frozen模型通常以文本生成任务为目标，例如图像描述生成。在推理过程中，该模型可以接受来自图像和文本嵌入的信息，并生成与图像内容相关联的描述。尽管Frozen在性能上相对适中，但它为后续开放式多模态零样本/少样本学习的发展奠定了重要基础。

MiniGPT系列进一步扩展了这一概念，使得同时接收文本和图像输入并产生相应文本输出成为可能。其中，MiniGPT-4通过简单线性投影层，将使用BLIP-2同样可视化编码器获得的图像嵌入，与Vicuna语言模型输入空间进行了有效对齐。由于这两个组件均经过预训练，因此MiniGPT-4仅需针对这个线性投影层进行培训，此过程分为两个阶段：第一阶段涉及五百万对来自Conceptual Caption、SBU及LAION等数据集的数据；第二阶段则包括400步指令微调。这一过程使得在较短时间内，仅凭有限计算资源即可完成高效培训。而MiniGPT-5在此基础上增加了图像生成功能，通过引入生成标记来创建新颖图像，这些标记被映射至特征向量并输入到冻结状态下的图像生成模块中，实现更强大的多模态处理能力。

继MiniGPT系列的创新之后，Bai等人推出的Qwen系列模型，包括Qwen-VL和Qwen-VL-Chat，在多模态交互领域取得了显著进展。这些模型结合了大型语言模型和视觉编码器，以增强对视觉和语言信息的处理能力。在Qwen模型架构中，LLM基于Qwen-7B进行初始化，而视觉编码器则采用ViT-bigG。模型通过单层交叉注意力模块将视觉表示压缩成固定长度的序列，这些序列随后被输入到LLM中。这一设计不仅提升了模型在处理视觉和语言信息时的效率，而且促进了多模态任务中更有效的信息交互，为多模态交互技术的发展提供了新的动力。

在多模态交互领域，Li等人提出的BLIP-2模型实现了预训练模型的高效利用，为图像到文本的转换提供了创新方案。BLIP-2通过预训练的冻结模型大幅缩短训练时间，利用视觉编码器如CLIP生成图像嵌入，并将其映射到大型语言模型如OPT的输入空间。模型的关键组件是Q-Former，一个约含100-200M参数的Transformer，它通过交叉注意力机制将随机初始化的“查询”向量与图像嵌入交互，并通过线性层投影到LLM输入空间。这种方法不仅提高了训练效率，还在多模态任务中实现了更精细的特征对齐，显著提升了图像理解和文本生成的紧密联系。

参考文献：

Tsimpoukeelli M., Menick J., Cabi S., et al. Multimodal Few-Shot Learning with Frozen Language Models[C]. Conference on Neural Information Processing Systems. 2021: 200-212.

**Zhu D., Chen J., Shen X., et al. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models[C]. International Conference on Learning Representations. 2024: ??????-???????.**

Brock A., De S., Smith S. L., et al. High-Performance Large-Scale Image Recognition Without Normalization[C]. International Conference on Machine Learning. 2021: 1059-1071.

Raffel C., Shazeer N., Roberts A., et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer[J]. Journal of Machine Learning Research, 2020, 21: 140:1-140:67.

Sharma P., Ding N., Goodman S., et al. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset for Automatic Image Captioning[C]. Meeting of the Association for Computational Linguistics. 2018: 2556-2565.

Gurevych I., Miyao Y. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset for Automatic Image Captioning[C]. Meeting of the Association for Computational Linguistics. 2018: 2556-2565.

Alayrac J. B., Donahue J., Luc P., et al. Flamingo: a Visual Language Model for Few-Shot Learning[C]. Conference on Neural Information Processing Systems. 2017: 5998-6008.

Li J., Li D., Savarese S., et al. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models[C]. International Conference on Machine Learning. 2023: 19730-19742.

**Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality.**

Ordnez V., Kulkarni G., Berg T. L. Im2Text: Describing Images Using 1 Million Captioned Photographs[C]. Conference on Neural Information Processing Systems. 2011: 1143-1151.

Schuhmann C., Vencu R., Beaumont R., et al. Laion-400m: Open Dataset of Clip-Filtered 400 Million Image-Text Pairs[EB/OL]. arXiv preprint arXiv:2111.02114, 2021.

Bai J., Bai S., Yang S., et al. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond[EB/OL]. arXiv preprint arXiv:2308.12966, 2023.

Bai J., Bai S., Chu Y., et al. Qwen Technical Report[EB/OL]. arXiv preprint arXiv:2309.16609, 2023.

## 1.4 生成式VLMs

生成式视觉语言模型是一类利用生成模型来处理和理解视觉和语言信息的模型。这些模型通常能够生成图像或文本，或者在给定一种模态的条件下生成另一种模态的内容。生成式模型的核心在于它们能够学习输入数据的分布，并根据这个分布生成新的数据实例。在视觉语言模型的上下文中，这意味着模型能够基于文本描述生成图像，或者基于图像生成描述性文本。

CoCa（Contrastive Captioner）是一种生成式文本生成器，它通过对比学习和生成损失来训练一个多模态文本解码器。CoCa接受图像编码器的输出和单模态文本解码器产生的文本嵌入作为输入，从而生成与图像内容相关的文本。CoCa在预训练阶段使用了大规模的图像和文本数据集，通过将标注的图像标签视为文本，从而学习图像和文本之间的关联。这使得它不仅能够进行图像描述生成，还能够执行其他多模态理解任务，如视觉问答任务，而无需进一步的适应性调整。

CM3leon是一个多模态生成模型，它专门用于文本到图像和图像到文本的生成任务。CM3leon借鉴了图像标记器和文本标记器的设计，将图像和文本编码为一系列的标记，然后通过一个Transformer模型来处理这些标记。CM3leon的训练分为两个阶段：第一阶段是检索增强的预训练，使用基于CLIP的编码器作为检索器来获取相关的多模态文档，并将这些文档添加到输入序列中；第二阶段是监督式微调（Supervised Fine-Tuning, SFT），在这个阶段，模型通过多任务指令调整来处理和生成不同模态的内容。这种两阶段训练方法使得CM3leon在多模态任务中表现出色，展示了自回归模型在处理文本和图像之间复杂交互方面的能力。

Chameleon是一系列混合模态基础模型，它们能够生成和推理混合了文本和图像内容的序列。Chameleon模型从一开始就设计为混合模态，使用统一的架构从头开始训练，处理所有模态——图像、文本和代码的混合。这种集成方法采用完全基于标记的表示，将图像和文本都转换为离散标记，使得相同的Transformer架构可以应用于图像和文本标记的序列，而无需为每种模态单独编码器。这种早期融合策略使得模型能够在不同模态之间无缝推理和生成，但也带来了优化稳定性和扩展性的技术挑战。

参考文献：

Yu J., Wang Z., Vasudevan V., et al. CoCa: Contrastive Captioners are Image-Text Foundation Models[EB/OL]. arXiv preprint arXiv:2205.01917, 2022.

Yu L., Shi B., Pasunuru R., et al. Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning[EB/OL]. arXiv preprint arXiv:2309.02591, 2023.

Gafni O., Polyak A., Ashual O., et al. Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors[C]. European Conference on Computer Vision. 2022: 89-106.

Zhang S., Roller S., Goyal N., et al. OPT: Open Pre-trained Transformer Language Models[EB/OL]. arXiv preprint arXiv:2205.01068, 2022.

Vaswani A., Shazeer N., Parmar N., et al. Attention is All you Need[C]. Advances in Neural Information Processing Systems. 2017: 5998-6008.

Team C. Chameleon: Chameleon: Mixed-Modal Early-Fusion Foundation Models[EB/OL]. arXiv preprint arXiv:2405.09818, 2024.

# 2 面向VLMs的对抗攻击方法

大纲：

介绍随着视觉语言模型的发展，对抗攻击的重点逐渐从早期的视觉模型发展至如今的视觉语言模型；介绍面向VLMs的对抗攻击的定义，从而介绍几种主流方法；介绍传统对抗样本的迁移性方向；介绍三种迁移性分类；

内容：



参考文献：

On the adversarial robustness of multi-modal foundation models.

On evaluating adversarial robustness of large vision-language models.

Are aligned neural networks adversarially aligned?

Visual adversarial examples jailbreak large language models.

Image hijacking: Adversarial images can control generative models at runtime.

## 2.1 跨模型迁移性增强方法

大纲：

介绍传统模型的跨模型迁移性增强的方法；介绍面向VLMs的跨模型迁移性增强的方法（主流+非主流）；

内容：

参考文献：



Set-level guidance attack: Boosting adversarial transferability of vision language pre-training models.

OT-Attack: Enhancing Adversarial Transferability of Vision-Language Models via Optimal Transport Optimization.

Improving Adversarial Transferability via Model Alignment.

## 2.2 跨提示迁移性增强方法

大纲：

介绍跨任务迁移性和跨提示迁移性的区别；介绍传统模型的跨任务迁移性的研究；介绍方法；

内容：



参考文献：



## 2.3 跨数据迁移性增强方法

大纲：

介绍传统任务的跨数据迁移性；介绍当前视觉语言模型的跨数据迁移性分为几类；介绍方法；

内容：

参考文献：







Towards adversarial attack on vision language pre-training models



Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory.

Transferable Diffusion-based Unrestricted Adversarial Attack on Pre-trained Vision-Language Models.



Stop Reasoning! When Multimodal LLM with Chain-of-Thought Reasoning Meets Adversarial Image.









给出对抗攻击的定义；介绍早期对抗攻击的逐步发展，主要介绍FGSM及其变种；

参考文献：

Intriguing Properties of Neural Networks

Explaining and Harnessing Adversarial Examples

Adversarial Examples in the Physical World

Boosting Adversarial Attacks With Momentum

Improving Transferability of Adversarial Examples With Input Diversity

Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks

Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks

Towards Deep Learning Models Resistant to Adversarial Attacks

## 2.2 面向视觉语言模型的对抗攻击策略

介绍随着发展对抗攻击拓展到了视觉语言模型；定义视觉语言模型上对抗攻击并解释它的多样性；介绍基础VLM对抗攻击设计方法；

参考文献：

Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs

Are Aligned Neural Networks Adversarially Aligned?

How Robust is Google’s Bard to Adversarial Image Attacks?

On Evaluating Adversarial Robustness of VLMs

On the Adversarial Robustness of Multi-Modal Foundation Models

Visual Adversarial Examples Jailbreak Aligned LLMs

An Image is Worth 1000 Lies

Image Hijacks

Adversarial Robustness for Visual Grounding of MLLMs

# 3 面向视觉语言模型的对抗样本迁移性研究

## 3.1 对抗样本迁移性的基础理论

初步概述对抗攻击并引出迁移性；强调对抗样本迁移性的研究重要性；介绍data augmentation、optimization technique、loss objective、model component四类迁移性攻击；

参考文献：

Delving into transferable adversarial examples and black-box attacks.

Improving transferability of adversarial examples with input diversity

Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks

Boosting adversarial attacks with momentum.

A method for unconstrained convex minimization problem with the rate of convergence o(1/kˆ 2).

Towards transferable targeted attack.

Improving transferability of adversarial patches on face recognition with generative models.

Transferable adversarial perturbations.

Task-generalizable adversarial attack based on perceptual metric.

## 3.2 面向视觉语言模型的对抗样本迁移性探索

介绍从视觉模型发展到视觉语言模型对抗样本迁移的重要性；介绍迁移性的主体：模型和提示；

参考文献：

Transferable multimodal attack on vision-language pre-training models. 

How Robust is Google’s Bard to Adversarial Image Attacks?

On Evaluating Adversarial Robustness of VLMs

An Image is Worth 1000 Lies

Adversarial Robustness for Visual Grounding of MLLMs

Visual Adversarial Examples Jailbreak Aligned LLMs

# 4 现存问题与发展趋势



# 5 结论

