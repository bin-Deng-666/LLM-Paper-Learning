近年来，Transformer架构在自然语言处理和计算机视觉领域的广泛应用，为视觉语言模型的发展奠定了坚实基础。视觉语言模型是一类多模态生成模型，能够从图像和文本中同时学习，其输入为图像和文本，输出为文本。借助Transformer架构强大的自注意力机制，这些模型实现了对多模态信息的高效处理和深度理解，在**图像描述生成、视觉问答和视觉定位**等多模态推理任务中展现出卓越的潜力。

<img src="C:\Users\DengBin\AppData\Roaming\Typora\typora-user-images\image-20241122191437037.png" alt="image-20241122191437037" style="zoom:50%;" />

然而，伴随着视觉语言模型的快速发展，对抗攻击研究也经历了重要的转变。早期的对抗攻击主要集中在单一模态的视觉模型上，通过微小的图像扰动来误导模型的视觉识别能力。然而，随着视觉语言模型在各种多模态任务中展现出卓越的能力，攻击者开始转向这些更为复杂的模型。由于视觉语言模型结合了视觉信息处理和自然语言理解的能力，这不仅提升了它们的实用性，也引入了新的安全挑战。**攻击者现在不仅可以考虑如何操纵图像和文本输入，还可以考虑结合多模态输入设计不同的攻击效果。**尤其是在医疗诊断等高风险场景中，视觉语言模型的应用对安全性提出了极高的要求。一旦这些关键领域内使用的视觉语言模型遭受对抗攻击，可能导致关键任务中的严重错误。例如，在医疗诊断中，模型在医学影像辅助诊断中出现偏差，可能导致误诊或漏诊，危及患者生命。

<img src="C:\Users\DengBin\AppData\Roaming\Typora\typora-user-images\image-20241122191812972.png" alt="image-20241122191812972" style="zoom:120%;" />

<img src="C:\Users\DengBin\AppData\Roaming\Typora\typora-user-images\image-20241122193411942.png" alt="image-20241122193411942" style="zoom:60%;" />

更令人担忧的是**对抗样本的迁移性**。这种现象表明，在一个模型上生成的对抗样本可以成功攻击结构不同的其他模型，甚至能误导不同的文本提示。这种迁移性不仅降低了攻击的门槛，同时也暴露了模型在安全性和可靠性上的缺陷。

为了应对这些挑战，我们需要更深入地研究对抗样本及其迁移性的机制。一方面，这有助于揭示视觉语言模型在实际应用中面临的威胁；另一方面，也为开发更鲁棒的防御策略奠定基础。



