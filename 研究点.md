Targeted attack：通过图像上的对抗扰动引导模型输出攻击者想要的目标文本

Untargeted attack：①通过图像上的对抗扰动引导模型尽量不输出正确的文本 ②在一定大小的图像扰动下尽量使得图像经过编码器之后的嵌入偏离原始嵌入

跨小任务

- 研究点1：在生成图像扰动的时候，正向优化文本引导模型输出正确文本，反向优化图像引导模型不输出正确文本，让两者在生成对抗图像的过程中具有一种对抗关系，从而让图像扰动更有攻击性

​        现在的多模态对抗攻击大多集中于视觉模态，忽略了文本模态在生成内容上的指导作用。例如，在图像描述任务中，[图像=“校运会上小明和小红在接力跑步“, prompt_1=“你能描述一下这张图像吗”, prompt_2=“这是一张校运会上拍的照片，你能描述一下这张图像吗”]，prompt_2在指导内容生成上比prompt_1更具优势。我们就可以利用文本这种指导作用，在生成图像扰动的时候，正向优化文本引导模型输出正确文本，反向优化图像引导模型不输出正确文本，让两者在生成对抗图像的过程中具有一种对抗关系，从而让攻击更有效率。
​        现在具有文本模态的研究：①通过图像扰动+文本对抗后缀同时引导模型输出目标文本 ②通过在多个文本prompt上进行训练并在其嵌入上进行扰动增大其嵌入空间（motivation不一样），从而使得图像扰动在prompt上具有迁移性 
方法：prompt tuning（额外添加一些图像实体描述） + 梯度优化（文本正向优化）

- 研究点2：语义上的untargeted attack

现在普遍的untargeted attack通过图像上的对抗扰动引导模型尽量不输出正确的文本，然而图像对应的正确文本是无穷多种的。例如，[图像=“一只小猫趴在桌子上“, prompt_1=“你能描述一下这张图像吗”]，图像的正确描述可以是“一只小猫趴在桌子上“，也可以是“桌子上趴了一只小猫”，降低了其中一种正确描述但模型仍然可能生成另一种正确描述。我这里的想法是在语义上实现untargeted attack，比如引导模型往没有“桌子”和“猫”等重要实体生成内容。

方法：实体提取+？







- 多模态对抗攻击

多模态对抗攻击是为了测试和展示这些模型在实际应用中的潜在安全风险和脆弱性。

通过构造和研究多模态对抗攻击，可以更全面地评估这些模型在实际对抗环境中的鲁棒性和安全性

- 现有方法的问题

①当前的多模态攻击研究通常集中于单一任务，例如图像描述或目标检测。这意味着，在现有的工作中，**攻击往往仅针对特定任务进行设计，而无法确保该图像无论在其他任务下都能成功对系统造成误导或干扰**。

②**当前的多模态攻击在特定任务上存在？**。例如，在目标检测任务中，排列targeted attack通过引导模型将物体定位到列表中下一个物体的位置，这种方法相对死板，缺乏灵活性。而在图像描述任务中，untargeted attack则尝试引导模型避免输出正确的文本描述，但由于图像的正确描述可能有无穷多种，尽管攻击成功避免了一种正确描述，模型仍有可能生成另一种符合图像的正确描述，这在一定程度上削弱了攻击的效果。

③当前的多模态攻击在计算图像扰动时，通常**忽视了文本模态在生成内容时的指导作用**。以图像描述任务为例，绝大多数攻击方法专注于通过最大化攻击者期望输出的文本概率以计算图像扰动。然而这些方法没有充分考虑文本模态在内容生成中的协同作用，导致图像扰动的效果可能不够理想。



