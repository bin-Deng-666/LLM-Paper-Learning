### Attention

- Flash-attention

<img src="/Users/dengbin/Library/Application Support/typora-user-images/image-20241028192052497.png" alt="image-20241028192052497" style="zoom:50%;" />

<center>CPU DRAM为内存；GPU HBM为显卡内存；GPU SRAM为显卡缓存</center>

现状：self-attention的复杂度是序列长度的二次方->序列过长时，算法复杂度很高
目的：加速注意力计算并减少内存占用
原理：使用平铺和重计算等技术，将QKV切分后从HBM加载到SRAM执行计算，然后将结果更新会HBM。

S和P与序列长度N的平方成正比，导致将中间结果S和P加载到SRAM中需要占用极大的GPU缓存和需要极大的时间。
QKV分块计算：

softmax分块计算：

- KV-Cache

