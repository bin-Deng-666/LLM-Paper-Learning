### Attention

- Flash-attention

<img src="/Users/dengbin/Library/Application Support/typora-user-images/image-20241028192052497.png" alt="image-20241028192052497" style="zoom:50%;" />

<center>CPU DRAM为内存；GPU HBM为显卡内存；GPU SRAM为显卡缓存</center>

现状：self-attention的复杂度是序列长度的二次方->序列过长时，算法复杂度很高
目的：加速注意力计算并减少内存占用
原理：使用平铺和重计算等技术，将QKV切分后从HBM加载到SRAM执行计算，然后将结果更新会HBM。

S和P与序列长度N的平方成正比，导致将中间结果S和P加载到SRAM中需要占用极大的GPU缓存和需要极大的时间。
QKV分块计算、softmax分块计算：1️⃣可以减少SRAM的占用 2️⃣分块计算可以跳过存储中间结果的过程，减少HBM的读写

- KV-Cache

在Self-attention中，因为attention mask会使得token的query向量只与其和其之前的token的key和value向量相乘，所以在生成下一个token的时候缓存之前token的key和value向量能够显著加速。

### 位置编码

- 正余弦编码

<img src="/Users/dengbin/Library/Application Support/typora-user-images/image-20241030172501309.png" alt="image-20241030172501309" style="zoom:70%;" />

- 旋转位置编码Rope

在self-attention的过程中纳入位置编码

<img src="/Users/dengbin/Library/Application Support/typora-user-images/image-20241030173638989.png" alt="image-20241030173638989" style="zoom:30%;" />

### 微调篇

- adapter

原理：针对每个Transformer层，增加两个Adapter结构（self-attention层和add+norm之间、feed-forward层和add+norm之间）。训练时，固定原来预训练模型参数不变，只对新增的Adapter结构和Layer norm层进行微调。
缺点：增加了额外的结构，这可能会导致推理时间的增加。

<img src="/Users/dengbin/Library/Application Support/typora-user-images/image-20241030190410526.png" alt="image-20241030190410526" style="zoom:40%;" />

<center>增加1个Adapter结构和1个Feed-forward层</center>

- prefix tuning

原理：在每层transfomer的输入的前面插入一段virtual tokens作为prefix，训练时只更新prefix部分的参数。
缺点：插入 prefix 后，原本用于实际输入内容的 token 数量减少，这可能导致模型在处理长序列或复杂任务时出现信息丢失或表达受限的问题。

<img src="/Users/dengbin/Library/Application Support/typora-user-images/image-20241030191502209.png" alt="image-20241030191502209" style="zoom:40%;" />

<center>基于soft prompt的微调</center>

- prompt tuning

原理：基于具体任务的需求，设计一个提示模板（Prompt Template）。该模板通常包含一些固定格式的文本，用于引导模型生成符合任务要求的输出。
缺点：任务高度定制化。

- p-tuning

原理：设计一个提示模板，该模板包含一系列虚拟标记（tokens）或嵌入向量，这些向量是待优化的参数。
缺点：

<img src="https://img-blog.csdnimg.cn/51d20d0ac1cd4044960b790ebca9d8fa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQlFXXw==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img" style="zoom:70%;" />

- p-tuning v2

原理：
缺点：

- lora

原理：
缺点：
